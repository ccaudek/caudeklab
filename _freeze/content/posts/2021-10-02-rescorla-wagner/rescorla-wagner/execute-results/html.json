{
  "hash": "35ebf2ba8f8478e5b7184dcb41493907",
  "result": {
    "markdown": "---\ntitle: \"Rescorla-Wagner\"\ndescription: |\n  Un semplice modello di apprendimento associativo.\nauthor:\n  - name: Corrado Caudek\n    url: {}\ndate: 10-02-2021\noutput:\n  distill::distill_article:\n    self_contained: false\ncategories:\n  - LM-51\n  - R\n---\n\n\n\n\n# Regola di Rescorla-Wagner\n\nIl modello di Rescorla-Wagner fornisce una regola di apprendimento che descrive come cambia la forza associativa durante il condizionamento pavloviano. Supponiamo di prendere uno stimolo inizialmente neutro (ad es. un tono) e di associarlo a un risultato che ha un valore intrinseco per l'organismo (ad es. un premio -- oppure una punizione). Col tempo l'organismo impara ad associare il tono al premio e risponderà al tono più o meno allo stesso modo in cui risponde al premio. In questo esempio il premio è lo stimolo incondizionato (US) e il tono è stimolo condizionato (SC).\n\nSecondo il modello Rescorla-Wagner, la regola per l'aggiornamento della forza associativa tra US e SC è basata sul divario tra l'aspettativa di ricompensa e il risultato che viene effettivamente ottenuto:\n\n$$\nv_{s,t} = v_{s,t-1} + \\alpha \\cdot (\\lambda_{t-1} - v_{s,t-1}),\n$$\ndove\n\n- $v_{s,t}$ è il valore dello stimolo $s$ nella prova $t$, che riflette l'aspettativa di una ricompensa,\n- $\\lambda_{t-1}$ è la ricompensa ricevuta nella prova $t-1$,\n- $\\alpha$ è il tasso di apprendimento.\n\nPertanto, il valore assegnato ad uno stimolo viene aggiornato in base all'errore di previsione (la differenza tra il feedback ricevuto $\\lambda_{t-1}$ e l'aspettativa di ricompensa $v_{s,t-1}$).\n\nIl tasso di apprendimento $\\alpha \\in [0, 1]$ determina quanto viene pesato questo errore di previsione nell'aggiornamento dell'aspettativa di ricompensa alla luce del feedback che è stato ottenuto.\n\n\n# Condizionamento\n\nPer chiarire il funzionamento della regola di Rescorla-Wagner la implementiamo in una funzione `R`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nupdate_rw <- function(value, alpha=0.15, lambda=1) {\n  value + alpha * (lambda - value)\n}\n```\n:::\n\n\nIn una prima simulazione costituita da una sequenza di 40 prove esaminiamo come varia  l'aspettativa di ricompensa dello stimolo $s$ nel tempo. Immaginiamo che il feedback ottenuto sia sempre pari ad una ricompensa ($\\lambda = 1$). Nella prima prova, il valore dello stimolo è inizializzato a zero.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_trials <- 40\nstrength <- numeric(n_trials)\nstrength\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0\n```\n:::\n\n```{.r .cell-code}\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw( strength[trial-1] )\n}\nprint(strength)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.0000000 0.1500000 0.2775000 0.3858750 0.4779937 0.5562947 0.6228505\n [8] 0.6794229 0.7275095 0.7683831 0.8031256 0.8326568 0.8577582 0.8790945\n[15] 0.8972303 0.9126458 0.9257489 0.9368866 0.9463536 0.9544006 0.9612405\n[22] 0.9670544 0.9719962 0.9761968 0.9797673 0.9828022 0.9853819 0.9875746\n[29] 0.9894384 0.9910226 0.9923692 0.9935139 0.9944868 0.9953138 0.9960167\n[36] 0.9966142 0.9971221 0.9975538 0.9979207 0.9982326\n```\n:::\n\n```{.r .cell-code}\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\")\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](rescorla-wagner_files/figure-html/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\nApplicando la regola di Rescorla-Wagner, il valore (ovvero, l'aspettativa di ricompensa) dello stimolo $s$, nel caso di feedback positivi, aumenta progressivamente fino a raggiungere l'asintoto di 1. Nella simulazione precedente abbiamo posto $\\alpha = 0.15$.  Con $\\alpha = 0.5$ otteniamo:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstrength <- numeric(n_trials)\n\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw(alpha = 0.5, strength[trial-1] )\n}\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](rescorla-wagner_files/figure-html/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\nÈ chiaro dunque che il parametro $\\alpha$ determina la velocità con la quale viene aggiornata l'aspettativa di ricompensa.\n\n\n# Estinzione\n\nConsideriamo ora l'estinzione dell'associazione che è stata appresa. In questa seconda simulazione, le prime 25 prove saranno identiche a quelle della simulazione precedente. In esse verrà sempre fornita una ricompensa ($\\lambda = 1)$. Le ultime 25 prove, invece, forniranno un feedback negativo, ovvero, $\\lambda = 0$ -- possiamo immaginare il feedback come l'assenza di premio.\n\nQuello che ci aspettiamo di vedere in questa situazione è che dopo la prova 25, quando il premio viene rimosso, la forza dell'associazione inizi a indebolirsi perché l'agente sta ora associando il CS con l'assenza di premio (cioè il parametro $\\lambda$ è sceso a zero e quindi l'associazione $v$ ritorna lentamente al valore iniziale). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_trials <- 50                \nstrength <- numeric(n_trials) \nlambda <- 1 # initial reward value \n\nfor(trial in 2:n_trials) {\n  \n  # remove the shock after trial 25\n  if(trial > 25) {\n    lambda <- 0\n  }\n  \n  # update associative strength on each trial\n  strength[trial] <- update_rw(\n    value = strength[trial-1],\n    lambda = lambda\n  )\n}\n\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](rescorla-wagner_files/figure-html/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\nL'estinzione è efficace nel rimuovere l'associazione, ma la sua efficacia richiede del tempo, non è immediata. Se ci fermiamo alla 35-esima prova, per esempio, allo stimolo $s$ sarà ancora associata una piccola aspettativa di ricompensa.\n\n\n# Regola soft-max\n\nUna volta attribuita una aspettativa di ricompensa agli stimoli, l'agente deve scegliere tra i diversi stimoli che sono presenti. Potrebbe sembrare ovvio scegliere, tra i vari stimoli presenti, quello a cui è associata l'aspettativa di ricompensa più altra (\"massimizzazione della probabilità\") in questo particolare compito. Ma gli organismi biologici non si comportano così. Piuttosto, tendono a scegliere più spesso lo stimolo a cui è associata l'aspettativa di ricompensa maggiore, ma non sempre. Ci sono marcate differenze individuali nella strategia di scelta che si colloca tra due estremi: un'estremo è quello in cui l'aspettativa di valore determina la scelta; l'altro estremo è quello in cui la scelta tra gli stimoli è puramente casuale (ovvero, non è in alcun modo determinata dall'aspettativa di ricompensa associata agli stimoli). \n\nPer descrivere il continuum tra queste due diverse strategie di scelta\n\nPer modellare il modo in cui gli agenti traducono i valori di aspettativa di ricompensa in una scelta, viene utilizzato un modello in grado di catturare queste diverse possibili strategie di scelta. A questo fine viene usata la cosiddetta equazione soft-max:\n\n$$\np(s) = \\frac{\\exp(\\beta v_s)}{\\sum_i \\exp(\\beta v_i)}.\n$$\nSe supponiamo che ci siano solo due stimoli, A e B, dove $v_B = 1 - v_A$, allora otteniamo la situazione seguente.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsoftmax <- function(beta, x) {\n  1 / (1 + exp(-beta * x))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta <- 5\nx <- seq(-1, 1, length.out = 100)\ny <- softmax(beta, x)\nplot(\n  x, \n  y, \n  type = 'l', \n  #ylim = c(0,1),\n  xlab = \"Valore (A) - valore (B)\",\n  ylab = \"p(scelta = A)\"\n)\n```\n\n::: {.cell-output-display}\n![](rescorla-wagner_files/figure-html/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\nSi noti che\n\n- La probabilità di scegliere lo stimolo A aumenta in modo monotono con la differenza di valore A - B.\n- La funzione softmax ci dice che l'agente sceglierà lo stimolo A la maggior parte delle volte quando $v_A > v_B$, ma non sempre.\n- Da qui deriva il termine 'softmax': l'agente sceglie lo stimolo con il valore maggiore la maggior parte delle volte (ma non sempre), quindi questa è una funzione di massimizzazione 'soft'.\n\n\n### Informazioni sulla sessione di lavoro {-}\n\n<details>\n<summary>\nSession Info\n</summary>\nSono qui fornite le informazioni sulla sessione di lavoro insieme all'elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  it_IT.UTF-8\n ctype    it_IT.UTF-8\n tz       Europe/Rome\n date     2022-11-22\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.4.1   2022-09-23 [1] CRAN (R 4.2.0)\n digest        0.6.30  2022-10-18 [1] CRAN (R 4.2.2)\n evaluate      0.18    2022-11-07 [1] CRAN (R 4.2.2)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.3   2022-10-21 [1] CRAN (R 4.2.2)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.2)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.18    2022-11-09 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.1)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.2)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\n</details>\n\n",
    "supporting": [
      "rescorla-wagner_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}