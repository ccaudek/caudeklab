[
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "\n About Me",
    "section": "",
    "text": "PhD in Experimental Psychology (1993)University of Virginia\nMA in Experimental Psychology (1991)University of Virginia\nMA in Philosophy (1986)University of Trieste"
  },
  {
    "objectID": "content/about.html#fa-university-corrent-position",
    "href": "content/about.html#fa-university-corrent-position",
    "title": "\n About Me",
    "section": "\n Corrent Position",
    "text": "Corrent Position\nI am currently an Associate Professor at the NEUROFARBA Department of the University of Florence, Italy."
  },
  {
    "objectID": "content/about.html#fa-search-minus-research-focus",
    "href": "content/about.html#fa-search-minus-research-focus",
    "title": "\n About Me",
    "section": "\n Research Focus",
    "text": "Research Focus\nI work on individual differences in cognitive style and strategy. Lately, I have focused on deficits in associative learning in obsessive-compulsive disorder, eating disorders, and depression. I uses mathematical models, computational simulations and statistical analysis to understand the whether cognitive inflexibility can be understood as a transdiagnostic construct relevant within clinical settings. The ultimate goal of my work is to help design better intervention strategies against psychological disorders, both for individual patients and on the population level."
  },
  {
    "objectID": "content/about.html#fa-address-book-contact-me",
    "href": "content/about.html#fa-address-book-contact-me",
    "title": "\n About Me",
    "section": "\n Contact Me",
    "text": "Contact Me\nYou can send me an email or directly message me on twitter."
  },
  {
    "objectID": "content/about.html#fa-location-dot-location",
    "href": "content/about.html#fa-location-dot-location",
    "title": "\n About Me",
    "section": "\n Location",
    "text": "Location\nDipartimento NEUROFARBA, Università degli Studi di Firenze\nVia di San Salvi n. 12, Complesso di S. Salvi, Padiglione 26, Firenze, 50139, Italy."
  },
  {
    "objectID": "content/about.html#fa-code-about-this-site",
    "href": "content/about.html#fa-code-about-this-site",
    "title": "\n About Me",
    "section": "\n About this site",
    "text": "About this site\nThis website was made with Quarto and R.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  it_IT.UTF-8\n ctype    it_IT.UTF-8\n tz       Europe/Rome\n date     2022-11-22\n pandoc   2.19.2\n Quarto   1.2.269\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n bayesplot   * 1.10.0  2022-11-16 [2] CRAN (R 4.2.0)\n brms        * 2.18.0  2022-09-19 [2] CRAN (R 4.2.0)\n broom       * 1.0.1   2022-08-29 [2] CRAN (R 4.2.0)\n cmdstanr    * 0.5.1   2022-04-24 [2] local\n crayon      * 1.5.2   2022-09-29 [2] CRAN (R 4.2.0)\n data.table  * 1.14.6  2022-11-16 [2] CRAN (R 4.2.0)\n datawizard  * 0.6.4   2022-11-19 [2] CRAN (R 4.2.2)\n DBI         * 1.1.3   2022-06-18 [2] CRAN (R 4.2.0)\n dbplyr      * 2.2.1   2022-06-27 [2] CRAN (R 4.2.0)\n downlit     * 0.4.2   2022-07-05 [2] CRAN (R 4.2.1)\n dplyr       * 1.0.10  2022-09-01 [2] CRAN (R 4.2.0)\n dtplyr      * 1.2.2   2022-08-20 [2] CRAN (R 4.2.0)\n duckdb      * 0.5.1   2022-09-20 [2] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [2] CRAN (R 4.2.0)\n ggradar     * 0.2     2022-11-22 [2] Github (ricardo-bion/ggradar@345535f)\n ggridges    * 0.5.4   2022-09-26 [2] CRAN (R 4.2.0)\n ggtext      * 0.1.2   2022-09-16 [2] CRAN (R 4.2.0)\n gt          * 0.8.0   2022-11-16 [2] CRAN (R 4.2.0)\n gtExtras    * 0.4.3   2022-11-05 [2] CRAN (R 4.2.0)\n gtools      * 3.9.3   2022-07-11 [2] CRAN (R 4.2.1)\n here        * 1.0.1   2020-12-13 [2] CRAN (R 4.2.0)\n lubridate   * 1.9.0   2022-11-06 [2] CRAN (R 4.2.0)\n miniUI      * 0.1.1.1 2018-05-18 [2] CRAN (R 4.2.0)\n patchwork   * 1.1.2   2022-08-19 [2] CRAN (R 4.2.0)\n posterior   * 1.3.1   2022-09-06 [2] CRAN (R 4.2.0)\n purrr       * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n quarto      * 1.2     2022-07-06 [2] CRAN (R 4.2.0)\n Rcpp        * 1.0.9   2022-07-08 [2] CRAN (R 4.2.0)\n readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n RSQLite     * 2.2.18  2022-10-04 [2] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n sf          * 1.0-9   2022-11-08 [2] CRAN (R 4.2.0)\n stringr     * 1.4.1   2022-08-20 [2] CRAN (R 4.2.0)\n styler      * 1.8.1   2022-11-07 [2] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [2] CRAN (R 4.2.0)\n tidybayes   * 3.0.2   2022-01-05 [2] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [2] CRAN (R 4.2.0)\n timechange  * 0.1.1   2022-11-04 [2] CRAN (R 4.2.0)\n xml2        * 1.3.3   2021-11-30 [2] CRAN (R 4.2.0)\n\n [1] /Users/corrado/_repositories/caudeklab/renv/library/R-4.2/x86_64-pc-linux-gnu\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/posts/index.html",
    "href": "content/posts/index.html",
    "title": " Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nRescorla-Wagner\n\n\n\n\n\n\n\nLM-51\n\n\nR\n\n\n\n\nUn semplice modello di apprendimento associativo.\n\n\n\n\n\n\nOct 2, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nSommatorie\n\n\n\n\n\nNotazione sommatoria.\n\n\n\n\n\n\nOct 1, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nScrivere la tesi con R Markdown\n\n\n\n\n\n\n\nL-24\n\n\nLM-51\n\n\nteaching\n\n\nR\n\n\n\n\nEvitare Word.\n\n\n\n\n\n\nOct 1, 2021\n\n\n0 min\n\n\n\n\n\n\n\n\nLa campana di Gauss\n\n\n\n\n\nUna prima occhiata alla distribuzione gaussiana.\n\n\n\n\n\n\nOct 1, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nIstruzioni prova finale L-24\n\n\n\n\n\n\n\nL-24\n\n\nteaching\n\n\n\n\nTutto quello che avreste voluto sapere sulla prova finale L-24 ma non avete mai osato chiedere.\n\n\n\n\n\n\nOct 1, 2021\n\n\n0 min\n\n\n\n\n\n\n\n\nIstogramma\n\n\n\n\n\n\n\nL-24\n\n\nPsicometria\n\n\nteaching\n\n\n\n\nLa rappresentazione grafica della distribuzione dei dati.\n\n\n\n\n\n\nOct 1, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\nBibliografia\n\n\n\n\n\n\n\nL-24\n\n\nLM-51\n\n\nteaching\n\n\n\n\nBibliografia della prova finale e della tesi di laurea magistrale.\n\n\n\n\n\n\nOct 1, 2021\n\n\n0 min\n\n\n\n\n\n\n\n\nManipolazione dei dati con dplyr\n\n\n\n\n\n\n\nR\n\n\nPsicometria\n\n\nL-24\n\n\n\n\nTutorial sulle funzionalità di dplyr per la manipolazione dei dati.\n\n\n\n\n\n\nSep 26, 2021\n\n\n0 min\n\n\n\n\n\n\n\n\nIntroduzione a R Markdown\n\n\n\n\n\n\n\nR\n\n\nteaching\n\n\n\n\nIntegrazione tra R e il linguaggio di markup Markdown.\n\n\n\n\n\n\nSep 26, 2021\n\n\n0 min\n\n\n\n\n\n\n\n\nLa struttura del progetto\n\n\n\n\n\n\n\nR\n\n\n\n\nSalvare e assegnare un nome ai documenti di un progetto.\n\n\n\n\n\n\nSep 25, 2021\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "content/pubs/index.html",
    "href": "content/pubs/index.html",
    "title": " Scientific Publications",
    "section": "",
    "text": "An up-to-date list of my publications can be found on the ORCID web page or on Google Scholar."
  },
  {
    "objectID": "content/teaching/index.html",
    "href": "content/teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Laboratorio di valutazione psicologica\nCourse Aims\n\nTo cultivate students’ practical communication skills, with particular emphasis on effective writing and speaking on psychology topics to a variety of audiences;\nTo provide students with a range of resources and skills for effective communication of complex material;\nTo provide students with the opportunity to undertake a practical project in science writing.\nSemesters Taught:\n\nSpring 2023\n\n\n Psicometria\nCourse Aims\nThis course provides students with a foundation in exploring data using the R programming language. Students will learn how to source, manage, transform, and explore a wide variety of data types. Students will also master the fundamental concepts for visualizing and communicating information contained in raw data. All analyses will be conducted to support reproducibility from raw data to results using RMarkdown. Teaching will involve interactive lectures with plenty of class time spent working on examples and coding. Students will be assessed through in-class quizzes, reading reflections, and exploratory projects. Throughout the semester, students will work on a research project of their own design to demonstrate mastery of the course’s topics. At the end of the semester, students will submit a final, reproducible report of their project along with an oral presentation of their findings. This course has a “flipped” classroom structure. Students will spend the majority of class time working through guiding practice exercises or working on their projects. Particular emphasis will be given to the fundamental concepts of Bayesian statistics for the analysis of psychological data.\nSemesters Taught:\n\nSpring 2023\n\n\n Costruzione e validazione di strumenti di misura dell’efficacia dell’intervento psicologico in neuropsicologia (curriculum: Assessment e intervento psicologici in neuropsicologia - E21)\nCourse Aims\nThis class is meant as a stimulus and guide for learning the basics of psychological and neuropsychological assessment. Topics include the basic concepts of neuropsychological assessment; the problem of neuropsychological deficit measurement; the specificity of the procedures of neuropsychological assessment; the interpretation of the outcome of a neuropsychological evaluation; the methodologies used to evaluate the effectiveness of psychological intervention in neuropsychology; The classic tests theory; factor analysis; the standard error of measurement; the standard error of estimation; teliability; item Analysis; factor scores; different forms of validity. At the end of the semester, students will submit a final, reproducible report of their project along with a 10-minute presentation of their findings. This course has a “flipped” classroom structure. Students will spend the majority of class time working through guiding practice exercises or working on their projects. To prepare for class, students must complete weekly assignments that involve watching and reviewing recorded lecture materials and answering related practice questions.\nSemesters Taught:\n\nSpring 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corrado Caudek",
    "section": "",
    "text": "Welcome to my personal website! I am an Associate Professor of Psychometrics and Quantitative Psychology at Università degli Studi di Firenze in Italy. I have a Ph.D. from University of Virginia with a specialization in Experimental Psychology and a Laurea (master’s degree) from Università degli Studi di Trieste in Italy, with a specialization in Experimental Psychology. Prior to joining the University of Florence, I was a Postdoctoral Researcher at the Center for Neural Science at New York University, a Postdoctoral Researcher at University of Trieste, and an Associate Professor at University of Trieste.\n\n\n\n\n\n\n\n\n\n\nI use this website to share information about my research and materials that can be useful to students. Thanks for visiting my site, and I am grateful for any feedback you may have!\n\n\n\nSono un goriziano e un orgoglioso relitto vagabondo dell'Università degli Studi di Trieste. Ho conseguito il dottorato di ricerca in psicologia sperimentale  presso la University of Virginia, a Charlottesville, nel 1993.\nSuccessivamente, ho completato un un post-dottorato presso l'Università degli Studi di Trieste e un post-dottorato presso il Center for Neural Science della New York University. Ho lavorato per diversi anni all'Università degli Studi di Trieste e attualmente sono professore associato all'Università degli Studi di Firenze, dipartimento NEUROFARBA, sezione di Psicologia."
  },
  {
    "objectID": "content/posts/2021-09-20-stimatori-distorti-e-non-distorti-della-varianza/stimatori-distorti-e-non-distorti-della-varianza.html",
    "href": "content/posts/2021-09-20-stimatori-distorti-e-non-distorti-della-varianza/stimatori-distorti-e-non-distorti-della-varianza.html",
    "title": "Stimatori distorti e non distorti della varianza",
    "section": "",
    "text": "Ci sono due formule per la varianza:\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})^2}{n}\n\\]\ne\n\\[\ns^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})^2}{n - 1}\n\\] La prima formula è quella di una statistica, e si pone il problema di fornire descrizione sintetica di una proprietà del campione – in questo caso, la varianza.\nLa seconda formula è quella di uno stimatore, e si pone l’obiettivo di descrivere, nella maniera più fedele possibile, una proprietà della popolazione – in questo caso, la varianza – utilizzando le informazioni presenti nel campione.\nLa prima formula, quella con \\(n\\) al denominatore, fallisce nello scopo che la seconda formula si propone di raggiungere (ovvero, l’obiettivo di descrivere le proprietà della popolazione). Infatti, in media, produrrà una stima troppo piccola. Usiamo una simulazione per esaminare questa faccenda.\nIniziamo a definire le proprietà della popolazione.\n\nmu <- 100 \nsigma <- 15\n\nDecidiamo, inoltre, di considerare campioni di ampiezza \\(n\\) = 30.\n\nsample_size <- 30\n\nNel caso di un singolo campione, per esempio, abiamo:\n\none_sample <- rnorm(sample_size, mu, sigma) \nmean(one_sample)\n\n[1] 99.30713\n\nvar(one_sample)\n\n[1] 290.815\n\n\nMa un singolo campione ci dice poco delle caratteristiche della “formula” che stiamo esaminando – quella che ha \\(n\\) al denominatore. Dato che è facile farlo con R, esaminiamo quello che succede quando consideriamo un milione di campioni:\n\nvar_distr <- replicate(\n  1e6,\n    var(\n      rnorm(sample_size, mu, sigma) \n    ) * (sample_size - 1) / sample_size\n)\n\nLa funzione rnorm() estrae un campione casuale di ampiezza sample_size (ovvero, 30) da una popolazione normale di media 100 e deviazione standard 15. La varianza della popolazione è dunque\n\n15^2\n\n[1] 225\n\n\nLa funzione var() * (sample_size - 1) / sample_size calcola la varianza delle 30 osservazioni di un campione utilizzando la prima formula (con \\(n\\) al denominatore).\nLa funzione replicate() ripete un milione di volte questi calcoli, ovvero calcola la varianza di un milione di campioni casuali di ampiezza 30 estratti da una popolazione normale di media 100 e varianza \\(15^2\\). Ciò significa che l’oggetto var_distr conterrà un milione di numeri: le varianze calcolate con la prima formula, per un milione di campioni casuali estratti dalla popolazione di riferimento.\nQuanto bene ha funzionato la prima formula? Ovviamente, alcune volte (cioé, per alcuni campioni) meglio, altre volte peggio. Il valore più piccolo che abbiamo ottenuto è\n\nmin(var_distr)\n\n[1] 31.01664\n\n\ne il valore più grande che abbiamo ottenuto è\n\nmax(var_distr)\n\n[1] 631.439\n\n\nIl valore medio – ovvero, il valore atteso del valore che si ottiene utilizzando la prima formula, è\n\nmean(var_distr)\n\n[1] 217.5273\n\n\nQuesto valore è chiaramente troppo piccolo. In altre parole, la prima formula, se venisse usata per stimare la varianza della popolazione produrrebbe una sottostima del valore del parametro cercato.\nSi può correggere questo errore sistematico?\nCertamente! E questo è lo scopo della seconda formula, quella con \\(n - 1\\) al denominatore. Ripetiamo la simulazione usando la seconda formula:\n\nvar_distr_c <- replicate(\n  1e6,\n  var(\n    rnorm(sample_size, mu, sigma) \n  )\n)\n\nIn questo caso, il valore atteso è\n\nmean(var_distr_c)\n\n[1] 225.062\n\n\nuguale al valore del parametro\n\n15^2\n\n[1] 225\n\n\nNella simulazione il risultato non è perfetto, ma si capisce che questa è, appunto, una simulazione: aumentando il numero delle ripetizioni si otterrebbe un valore sempre più simile al valore teorico. Ma non è necessario fare questo. La conclusione è chiara: la seconda formula ci fornisce uno stimatore corretto (ovvero, privo di errore sistematico) della varianza della popolazione. Per questa ragione dividiamo per (\\(n\\) - 1)."
  },
  {
    "objectID": "content/posts/2021-09-24-facebook/facebook.html",
    "href": "content/posts/2021-09-24-facebook/facebook.html",
    "title": "Facebook",
    "section": "",
    "text": "È attiva la pagina facebook del laboratorio. Contiene informazioni su progetti in corso e presentazioni dei laureandi. È una creazione dei torocinanti del laboratorio."
  },
  {
    "objectID": "content/posts/2021-09-25-la-struttura-del-progetto/la-struttura-del-progetto.html",
    "href": "content/posts/2021-09-25-la-struttura-del-progetto/la-struttura-del-progetto.html",
    "title": "La struttura del progetto",
    "section": "",
    "text": "Di seguito riporto il link ad un breve video-tutorial che cerca di rispondere alle seguenti domande: come organizzo tutto il materiale che fa parte di un progetto? Quali sono i principi che devo seguire per assegnare i nomi ai file? Come devo organizzare i file nelle cartelle? L’obiettivo è quello di immagazzinare sul computer il lavoro che abbiamo fatto in maniera tale che, in futuro, sarà facile recuperare quello che ci serve. E in modo tale che sia facile lavorare al nostro progetto nel presente.\nLe considerazioni che faccio fanno riferimento a delle raccomandazioni che “sono nell’aria”, ovvero, che sono condivise da molte persone. Questo materiale è stato organizzato in una serie di video youtube da Danielle Navarro e io mi limito a commentare brevemente quello che lei dice. Ovviamente, invece di seguire il mio video-tutorial, potete guardare direttamente i video di Danielle Navarro.\nUna cosa che mi sono dimenticato di dire nel mio video, e che invece è presente nei video youtube, è la seguente: dove dobbiamo salvare la cartella che contiene tutto il materiale di un progetto? Pessime risposte a questa domanda sono: il Desktop oppure la cartella di Download. Peggio di così non si può fare. Perché sia il Desktop sia la cartella Download contengono informazioni transienti, ovvero file che butterete via ad un certo punto – presto, si spera. Invece la cartella che contiene il vostro progetto contiene il frutto del vostro lavoro – e certamente non volete cancellarla per sbaglio. Quindi, un’idea molto migliore è quella di salvare la cartella del progetto nella cloud, ovvero, sul vostro comupter in cartelle come OneDrive o Dropbox.\nSpero che questo sia utile.\nInformazioni sulla sessione di lavoro\n\nSession Info\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  it_IT.UTF-8\n ctype    it_IT.UTF-8\n tz       Europe/Rome\n date     2022-11-22\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.4.1   2022-09-23 [1] CRAN (R 4.2.0)\n digest        0.6.30  2022-10-18 [1] CRAN (R 4.2.2)\n evaluate      0.18    2022-11-07 [1] CRAN (R 4.2.2)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.3   2022-10-21 [1] CRAN (R 4.2.2)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.2)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.18    2022-11-09 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.1)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.2)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/posts/2021-09-26-introduzione-a-r-markdown/introduzione-a-r-markdown.html",
    "href": "content/posts/2021-09-26-introduzione-a-r-markdown/introduzione-a-r-markdown.html",
    "title": "Introduzione a R Markdown",
    "section": "",
    "text": "Posto qui il link ad un breve video-tutorial sull’uso di R Markdown.\nLa migliore descrizione del flusso di lavoro (workflow) con R Markdown è fornita in questo capitolo di R for Data Science.\nPer chi ha bisogno di un’introduzione, risultano sicuramente utili anche le slide di Danielle Navarro."
  },
  {
    "objectID": "content/posts/2021-09-26-manipolazione-dei-dati-con-dplyr/manipolazione-dei-dati-con-dplyr.html",
    "href": "content/posts/2021-09-26-manipolazione-dei-dati-con-dplyr/manipolazione-dei-dati-con-dplyr.html",
    "title": "Manipolazione dei dati con dplyr",
    "section": "",
    "text": "Aggiungo qui il link ad un video-tutorial che ho preparato relativamente alla manipolaizone dei dati usando le funzioni del pacchetto dplyr.\nNel video non faccio altro che commentare un tutorial predisposto da Allison Horst e disponibile seguendo questo link. Il tutorial di Allison Horst è fatto benissimo e non vedo ragioni di tradurlo o cambiarlo in qualche modo. Inoltre, se andate nella pagina web che ho indicato sopra, potete anche fare degli esercizi che vi consentono di verificare se avete capito come utilizzare in pratica le istruzioni R che vengono discusse – le risposte agli esercizi sono immediatamente disponibili il che rende il tutorial di Allison Horst un utilissimo strumento di apprendimento. Buon divertimento!"
  },
  {
    "objectID": "content/posts/2021-10-01-bibliografia/bibliografia.html",
    "href": "content/posts/2021-10-01-bibliografia/bibliografia.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Ho creato un breve video con le istruzioni per creare la bibliografia delle tesi di laurea. Ci sono tanti tutorial sul web per affrontare questo problema: le semplici considerazioni che fornisco qui sono un possibile punto di partenza.\nAl di là delle cose che dico qui, consiglio fortemente tutti i laureandi, triennali e magistrali, di scrivere la tesi di laurea in R Markdown, usando le indicazioni fornite in un altro post e, soprattutto, di utilizzare bibtex per la bibliografia, in modo tale essere sicuri di ottenere il risultato corretto senza doversi preoccupare di applicare le (complicate) regole APA – i ricercatori fanno così quando scrivono un articolo."
  },
  {
    "objectID": "content/posts/2021-10-01-istogramma/istogramma.html",
    "href": "content/posts/2021-10-01-istogramma/istogramma.html",
    "title": "Istogramma",
    "section": "",
    "text": "In questo tutorial ci poniamo il problema di costruire un istrogramma utilizzando la funzione ggplot() del pacchetto ggplot2 in R. Vedremo quali sono i limiti degli istogrammi. Concluderemo introducendo una rappresentazione alternativa, la densità della frequenza dei dati, la quale attenua i limiti degli istogrammi."
  },
  {
    "objectID": "content/posts/2021-10-01-istogramma/istogramma.html#istogramma",
    "href": "content/posts/2021-10-01-istogramma/istogramma.html#istogramma",
    "title": "Istogramma",
    "section": "\n2.1 Istogramma",
    "text": "2.1 Istogramma\nCreiamo ora un istogramma usando i valori x.\nQual è l’altezza della barra in corrispondeza dell’intervallo (0,2]?\nLa base è pari a 2 e l’area è 5/8. Dunque l’altezza è\n\n(5/8) / 2\n\n[1] 0.3125\n\n\nUsiamo ggplot()\n\nx %>% \n  as.data.frame() %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 2, 4, 6, 8) \n  ) \n\n\n\n\n\n\n\nQuesto ci conferma che, di default, ggplot() usa intervalli chiusi a destra.\nCambiamo ora il default e specifichiamo intervalli chiusi a sinistra:\n\nx %>% \n  as.data.frame() %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 2, 4, 6, 8),\n    closed = \"left\"\n  )"
  },
  {
    "objectID": "content/posts/2021-10-01-istogramma/istogramma.html#visualizzazione-con-ggplot",
    "href": "content/posts/2021-10-01-istogramma/istogramma.html#visualizzazione-con-ggplot",
    "title": "Istogramma",
    "section": "\n5.1 Visualizzazione con ggplot()\n",
    "text": "5.1 Visualizzazione con ggplot()\n\n\n\n\n\np1 <- bysubj %>% \n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1) # il valore BDI-II massimo è 44\n  ) +\n  scale_x_continuous(breaks=c(0, 13.5, 19.5, 28.5, 44.1)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\np1\n\n\n\nFigura 1: Istogramma delle frequenze relative creato con ggplot().\n\n\n\n\nÈ più comune, però, utilizzare classi di ampiezza uguale.\n\n\n\n\np2 <- bysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  scale_x_continuous(breaks=c(0.00,  7.35, 14.70, 22.05, 29.40, 36.75, 44.10)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\",\n    caption = \"Fonte: Zetsche, Buerkner, & Renneberg (2020)\"\n  )\n\n\np1 + p2"
  },
  {
    "objectID": "content/posts/2021-10-01-istogramma/istogramma.html#limite-dellistogramma",
    "href": "content/posts/2021-10-01-istogramma/istogramma.html#limite-dellistogramma",
    "title": "Istogramma",
    "section": "\n5.2 Limite dell’istogramma",
    "text": "5.2 Limite dell’istogramma\nCome abbiamo notato sopra, uno dei limiti degli istogrammi è che il profilo dell’istogramma è arbitrario: a seconda del numero e dei limiti delle classi che vengono scelte, cambiano sia il numero che la forma delle barre dell’istogramma."
  },
  {
    "objectID": "content/posts/2021-10-01-istruzioni-prova-finale-l-24/istruzioni-prova-finale-l-24.html",
    "href": "content/posts/2021-10-01-istruzioni-prova-finale-l-24/istruzioni-prova-finale-l-24.html",
    "title": "Istruzioni prova finale L-24",
    "section": "",
    "text": "Mediante questo link potete accedere ad un video in cui rispondo a tutte le possibili domande che potreste avere su questo argomento. Descriverò la procedura che consiglio per realizzare l’elaborato finale e per preparare la presentazione orale. Una volta scritto l’elaborato finale seguendo queste istruzioni, potete iniziare a lavorare alla presentazione orale. Sulla presentazione orale riceverete poi altri feedback negli incontri settimanali con i laureandi che saranno specificamente dedicati a questo tema."
  },
  {
    "objectID": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html",
    "href": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html",
    "title": "La campana di Gauss",
    "section": "",
    "text": "Per iniziare, carichiamo i pacchetti necessari."
  },
  {
    "objectID": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html#la-funzione-di-ripartizione",
    "href": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html#la-funzione-di-ripartizione",
    "title": "La campana di Gauss",
    "section": "\n1.1 La funzione di ripartizione",
    "text": "1.1 La funzione di ripartizione\n\ncurve(\n  pnorm(x), \n  xlim = c(-3.5, 3.5), \n  ylab = \"Probabilità\", \n  main = \"Funzione cumulativa della normale standardizzata\"\n)"
  },
  {
    "objectID": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html#quantili-e-densità",
    "href": "content/posts/2021-10-01-la-campana-di-gauss/la-campana-di-gauss.html#quantili-e-densità",
    "title": "La campana di Gauss",
    "section": "\n1.2 Quantili e densità",
    "text": "1.2 Quantili e densità\nDefiniamo i seguenti quantili e calcoliamo la densità corrispondente per il caso della normale standardizzata:\n\nquants <- c(-1.96, 0, 1.96)\ngauss(quants, 0, 1)\n\n[1] 0.05844094 0.39894228 0.05844094\n\n\nLo stesso risultato si ottene con\n\ndnorm(quants, 0, 1)\n\n[1] 0.05844094 0.39894228 0.05844094"
  },
  {
    "objectID": "content/posts/2021-10-01-scrivere-la-tesi-con-r-markdown/scrivere-la-tesi-con-r-markdown.html",
    "href": "content/posts/2021-10-01-scrivere-la-tesi-con-r-markdown/scrivere-la-tesi-con-r-markdown.html",
    "title": "Scrivere la tesi con R Markdown",
    "section": "",
    "text": "Seguendo questo link potete trovare un video-tutorial sull’uso di R Markdown per la scrittura della tesi di laurea. Il materiale che ho predisposto può essere scaricato selezionando questo link. Buon lavoro!"
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html",
    "title": "Sommatorie",
    "section": "",
    "text": "Il simbolo \\(\\sum_{i=1}^{\\infty}\\) indica che dobbiamo assegnare al numero intero \\(i\\) tutti i suoi valori \\(1, 2, 3, ...\\) ed eseguire la somma dei termini. (Jean-Baptiste Joseph Fourier)\nLe somme si incontrano costantemente in svariati contesti matematici e statistici quindi abbiamo bisogno di una notazione adeguata che ci consenta di gestirle. Si veda, ad esempio, Wikipedia."
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-1",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-1",
    "title": "Sommatorie",
    "section": "\n2.1 Proprietà 1",
    "text": "2.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa: [ {i=1}^{n} a = {n~} = n a. ]"
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-2-proprietà-distributiva",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-2-proprietà-distributiva",
    "title": "Sommatorie",
    "section": "\n2.2 Proprietà 2 (proprietà distributiva)",
    "text": "2.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con [ {i=1}^{n} a x_i = a x_1 + a x_2 + + a x_n ] è possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere [ {i=1}^{n} a x_i = a _{i=1}^{n} x_i. ]"
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-3-proprietà-associativa",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-3-proprietà-associativa",
    "title": "Sommatorie",
    "section": "\n2.3 Proprietà 3 (proprietà associativa)",
    "text": "2.3 Proprietà 3 (proprietà associativa)\nNel caso in cui [ {i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + (a + x_n) ] si ha che [ {i=1}^{n} (a + x_i) = n a + {i=1}^{n} x_i. ] È dunque chiaro che in generale possiamo scrivere [ {i=1}^{n} (x_i + y_i) = {i=1}^{n} x_i + {i=1}^{n} y_i. ]"
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-4",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-4",
    "title": "Sommatorie",
    "section": "\n2.4 Proprietà 4",
    "text": "2.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio, [ {i=1}^{n} x_i^2 = x_1^2 + x_2^2 + + x_n^2 ({i=1}^{n} x_i )^2. ]"
  },
  {
    "objectID": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-5",
    "href": "content/posts/2021-10-01-sommatorie/sommatorie.html#proprietà-5",
    "title": "Sommatorie",
    "section": "\n2.5 Proprietà 5",
    "text": "2.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo: [ _{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + + x_n y_n, ] infatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\)."
  },
  {
    "objectID": "content/posts/2021-10-02-rescorla-wagner/rescorla-wagner.html",
    "href": "content/posts/2021-10-02-rescorla-wagner/rescorla-wagner.html",
    "title": "Rescorla-Wagner",
    "section": "",
    "text": "1 Regola di Rescorla-Wagner\nIl modello di Rescorla-Wagner fornisce una regola di apprendimento che descrive come cambia la forza associativa durante il condizionamento pavloviano. Supponiamo di prendere uno stimolo inizialmente neutro (ad es. un tono) e di associarlo a un risultato che ha un valore intrinseco per l’organismo (ad es. un premio – oppure una punizione). Col tempo l’organismo impara ad associare il tono al premio e risponderà al tono più o meno allo stesso modo in cui risponde al premio. In questo esempio il premio è lo stimolo incondizionato (US) e il tono è stimolo condizionato (SC).\nSecondo il modello Rescorla-Wagner, la regola per l’aggiornamento della forza associativa tra US e SC è basata sul divario tra l’aspettativa di ricompensa e il risultato che viene effettivamente ottenuto:\n\\[\nv_{s,t} = v_{s,t-1} + \\alpha \\cdot (\\lambda_{t-1} - v_{s,t-1}),\n\\] dove\n\n\n\\(v_{s,t}\\) è il valore dello stimolo \\(s\\) nella prova \\(t\\), che riflette l’aspettativa di una ricompensa,\n\n\\(\\lambda_{t-1}\\) è la ricompensa ricevuta nella prova \\(t-1\\),\n\n\\(\\alpha\\) è il tasso di apprendimento.\n\nPertanto, il valore assegnato ad uno stimolo viene aggiornato in base all’errore di previsione (la differenza tra il feedback ricevuto \\(\\lambda_{t-1}\\) e l’aspettativa di ricompensa \\(v_{s,t-1}\\)).\nIl tasso di apprendimento \\(\\alpha \\in [0, 1]\\) determina quanto viene pesato questo errore di previsione nell’aggiornamento dell’aspettativa di ricompensa alla luce del feedback che è stato ottenuto.\n\n2 Condizionamento\nPer chiarire il funzionamento della regola di Rescorla-Wagner la implementiamo in una funzione R:\n\nupdate_rw <- function(value, alpha=0.15, lambda=1) {\n  value + alpha * (lambda - value)\n}\n\nIn una prima simulazione costituita da una sequenza di 40 prove esaminiamo come varia l’aspettativa di ricompensa dello stimolo \\(s\\) nel tempo. Immaginiamo che il feedback ottenuto sia sempre pari ad una ricompensa (\\(\\lambda = 1\\)). Nella prima prova, il valore dello stimolo è inizializzato a zero.\n\nn_trials <- 40\nstrength <- numeric(n_trials)\nstrength\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0\n\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw( strength[trial-1] )\n}\nprint(strength)\n\n [1] 0.0000000 0.1500000 0.2775000 0.3858750 0.4779937 0.5562947 0.6228505\n [8] 0.6794229 0.7275095 0.7683831 0.8031256 0.8326568 0.8577582 0.8790945\n[15] 0.8972303 0.9126458 0.9257489 0.9368866 0.9463536 0.9544006 0.9612405\n[22] 0.9670544 0.9719962 0.9761968 0.9797673 0.9828022 0.9853819 0.9875746\n[29] 0.9894384 0.9910226 0.9923692 0.9935139 0.9944868 0.9953138 0.9960167\n[36] 0.9966142 0.9971221 0.9975538 0.9979207 0.9982326\n\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\")\npoints(1:n_trials, strength)\n\n\n\n\n\n\n\nApplicando la regola di Rescorla-Wagner, il valore (ovvero, l’aspettativa di ricompensa) dello stimolo \\(s\\), nel caso di feedback positivi, aumenta progressivamente fino a raggiungere l’asintoto di 1. Nella simulazione precedente abbiamo posto \\(\\alpha = 0.15\\). Con \\(\\alpha = 0.5\\) otteniamo:\n\nstrength <- numeric(n_trials)\n\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw(alpha = 0.5, strength[trial-1] )\n}\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n\n\n\n\n\n\n\nÈ chiaro dunque che il parametro \\(\\alpha\\) determina la velocità con la quale viene aggiornata l’aspettativa di ricompensa.\n\n3 Estinzione\nConsideriamo ora l’estinzione dell’associazione che è stata appresa. In questa seconda simulazione, le prime 25 prove saranno identiche a quelle della simulazione precedente. In esse verrà sempre fornita una ricompensa (\\(\\lambda = 1)\\). Le ultime 25 prove, invece, forniranno un feedback negativo, ovvero, \\(\\lambda = 0\\) – possiamo immaginare il feedback come l’assenza di premio.\nQuello che ci aspettiamo di vedere in questa situazione è che dopo la prova 25, quando il premio viene rimosso, la forza dell’associazione inizi a indebolirsi perché l’agente sta ora associando il CS con l’assenza di premio (cioè il parametro \\(\\lambda\\) è sceso a zero e quindi l’associazione \\(v\\) ritorna lentamente al valore iniziale).\n\nn_trials <- 50                \nstrength <- numeric(n_trials) \nlambda <- 1 # initial reward value \n\nfor(trial in 2:n_trials) {\n  \n  # remove the shock after trial 25\n  if(trial > 25) {\n    lambda <- 0\n  }\n  \n  # update associative strength on each trial\n  strength[trial] <- update_rw(\n    value = strength[trial-1],\n    lambda = lambda\n  )\n}\n\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n\n\n\n\n\n\n\nL’estinzione è efficace nel rimuovere l’associazione, ma la sua efficacia richiede del tempo, non è immediata. Se ci fermiamo alla 35-esima prova, per esempio, allo stimolo \\(s\\) sarà ancora associata una piccola aspettativa di ricompensa.\n\n4 Regola soft-max\nUna volta attribuita una aspettativa di ricompensa agli stimoli, l’agente deve scegliere tra i diversi stimoli che sono presenti. Potrebbe sembrare ovvio scegliere, tra i vari stimoli presenti, quello a cui è associata l’aspettativa di ricompensa più altra (“massimizzazione della probabilità”) in questo particolare compito. Ma gli organismi biologici non si comportano così. Piuttosto, tendono a scegliere più spesso lo stimolo a cui è associata l’aspettativa di ricompensa maggiore, ma non sempre. Ci sono marcate differenze individuali nella strategia di scelta che si colloca tra due estremi: un’estremo è quello in cui l’aspettativa di valore determina la scelta; l’altro estremo è quello in cui la scelta tra gli stimoli è puramente casuale (ovvero, non è in alcun modo determinata dall’aspettativa di ricompensa associata agli stimoli).\nPer descrivere il continuum tra queste due diverse strategie di scelta\nPer modellare il modo in cui gli agenti traducono i valori di aspettativa di ricompensa in una scelta, viene utilizzato un modello in grado di catturare queste diverse possibili strategie di scelta. A questo fine viene usata la cosiddetta equazione soft-max:\n\\[\np(s) = \\frac{\\exp(\\beta v_s)}{\\sum_i \\exp(\\beta v_i)}.\n\\] Se supponiamo che ci siano solo due stimoli, A e B, dove \\(v_B = 1 - v_A\\), allora otteniamo la situazione seguente.\n\nsoftmax <- function(beta, x) {\n  1 / (1 + exp(-beta * x))\n}\n\n\nbeta <- 5\nx <- seq(-1, 1, length.out = 100)\ny <- softmax(beta, x)\nplot(\n  x, \n  y, \n  type = 'l', \n  #ylim = c(0,1),\n  xlab = \"Valore (A) - valore (B)\",\n  ylab = \"p(scelta = A)\"\n)\n\n\n\n\n\n\n\nSi noti che\n\nLa probabilità di scegliere lo stimolo A aumenta in modo monotono con la differenza di valore A - B.\nLa funzione softmax ci dice che l’agente sceglierà lo stimolo A la maggior parte delle volte quando \\(v_A > v_B\\), ma non sempre.\nDa qui deriva il termine ‘softmax’: l’agente sceglie lo stimolo con il valore maggiore la maggior parte delle volte (ma non sempre), quindi questa è una funzione di massimizzazione ‘soft’.\n\nInformazioni sulla sessione di lavoro\n\nSession Info\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  it_IT.UTF-8\n ctype    it_IT.UTF-8\n tz       Europe/Rome\n date     2022-11-22\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.4.1   2022-09-23 [1] CRAN (R 4.2.0)\n digest        0.6.30  2022-10-18 [1] CRAN (R 4.2.2)\n evaluate      0.18    2022-11-07 [1] CRAN (R 4.2.2)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n htmltools     0.5.3   2022-07-18 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.3   2022-10-21 [1] CRAN (R 4.2.2)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.2)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.18    2022-11-09 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.1)\n stringr       1.4.1   2022-08-20 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.2)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]